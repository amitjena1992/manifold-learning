\chapter{Experiments}
\label{sec:experiments}

This chapter will cover the experiments implemented throughout this project, which focused on reducing data sets with the presented algorithms and evaluating these reductions. Once again, it is quite difficult to evaluate reductions, but a set of metrics were adopted to attempt to understand them as much as possible.

First, let $\vect X_{n \times f}$ be a data set with $n$ samples and $f$ features, and $\vect D \subset \mathbb{N} \mid d \in \vect D \implies d \le f$ a  set of dimensions to which $\vect X$ should be reduced. Additionally, consider the score to be the class prediction accuracy or the coefficient of determination $R^2$, depending on the nature (classification or regression) of the problem of interest. Then, the sequence bellow represents a default format followed by all experiments:

\begin{enumerate}
	\item The data set $\vect X$ is loaded and its first three features are plotted, given an initial idea of how the data is distributed for those features.
	\item $\vect X$ is reduced to 3, 2 and 1 dimensions. Each reduction is plotted for visualization.
	\item Kruskal's stress is calculated for $\vect X$.
	\item In an attempt to evaluate local dissimilarity preservation, a 1-NN classifier or regressor is trained with 80\% of $\vect X$'s samples and tested with the other 20\%. The score is kept for future reference.
	\item Grid search is executed over $\vect X$ using a SVM classifier or regressor. This step evaluates how a ``real world" algorithm would perform over the original data set.
	\item for $d \in \vect D$:
	\begin{enumerate}
		\item $\vect X$ is embedded into the $\mathbb{R}^d$, creating the data set $\vect Y_{n \times d}$.
		\item Kruskal's stress is calculated for $\vect Y$.
		\item A 1-NN classifier or regressor is trained with the same samples used in step 4, and tested with all the others. The score is compared to the one retrieved in step 4.
		\item Grid Search is executed over $\vect Y$ and the score is compared to the one presented in step 5.
	\end{enumerate}
\end{enumerate}
\newpage

\section{K Data Set}
\label{experiments:k}

The synthetic data set $\vect K$, with 1000 samples and 2 features.
\newline\newline

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{datasets/r}
	\caption{The $\vect K$ data set.}
\end{figure}

\newpage
\paragraph{Reducing K Data Set With the PCA Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	
	\includegraphics[width=\linewidth]{experiments/pca_k}
	\caption{The reductions of $\vect K$ to 2 and 1 dimension, respectively.}
	\label{fig:dsrpca}
\end{figure}

Figure \ref{fig:dsrpca} illustrates the results of PCA algorithm application over $\vect K$ data set. Notice that, for the second application, it correctly chose to discard the vertical dimension, as the samples offer less variability in this component.

\begin{table}[H]
	\centering
	\begin{tabular}{|p{.25\linewidth}|p{.20\linewidth}|p{.20\linewidth}|p{.20\linewidth}|}
		\hline
		& \textbf{Original $\mathbb{R}^2$} & \textbf{$\mathbb{R}^2$} & \textbf{$\mathbb{R}$} \\\hline
		\textbf{1-NN} & .99 & .99 & 1 \\\hline
		\textbf{Score} & .982 & .982 & .994 \\\hline
		\textbf{Best parameters} & 'C': 1000, 'k': 'linear' & 'C': 1000, 'k': 'linear' & 'C': 1000, 'g': 10, 'k': rbf \\\hline
		\textbf{GridSearch time} & 2.55 s s & 2.93 s & 2.94 s \\\hline
		\textbf{Reduction time} & - & 1.97 s & 1.93 s \\\hline
		\textbf{Kruskal's stress} & - & 0 & .0399 \\\hline
		\textbf{Data size} & 15.62 KB & 15.62 KB & 7.81 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for $\vect K$.}
\end{table}

From the table above, one can see that, given the linear nature of $\vect K$, PCA was an adequate method to reduce it, as both 1-NN regressor and SVM created very accurate models. It is also possible to observe how Kruskal's stress might not be an appropriate quality measurement, given a specific domain. Indeed, $\vect K$'s stress increase resulted from the removal of one of the components did not imply on prediction accuracy decrease.

\clearpage
\section{The Iris Flower Data Set}

The Iris Flower data set, as presented in section \ref{irisdataset}, containing 150 samples and 4 features.
\newline\newline

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{datasets/iris}
	\caption{The Iris Flower data set.}
\end{figure}

\newpage
\paragraph{Reducing the Iris Flower Data Set With the PCA Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/pca_iris}
	\caption{The reductions of Iris Flower to 3, 2, and 1 dimensions, respectively, using the PCA algorithm.}
	\label{fig:dsirispca}
\end{figure}

One can infer from figure \ref{fig:dsirispca} that classes are still somewhat organized in different clusters, even in the reductions for 2 and 1 dimensions. The Application of PCA therefore results in fairly good visualizations.

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.25\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^3$ & $\mathbb{R}^2$ & $\mathbb{R}$ \\\hline
		\textbf{1-NN} & 1 & 1 & 1 & .87 \\\hline
		\textbf{Score} & .99 & .98 & .97 & .95 \\\hline
		\textbf{Best parameters} & g: .01, k: rbf, C: 100 & k: linear, C: 10 & g: 10, k: rbf, C: 1 & g: .1, k: sigmoid, C: 1 \\\hline
		\textbf{GridSearch time} & 2.14 s & 2.64 s & 2.42 s & 2.64 s \\\hline
		\textbf{Reduction time} & - & .17 s & .17 s & .16 s \\\hline
		\textbf{Kruskal's stress} & - & .01 & .04 & .11 \\\hline
		\textbf{Data size} & 4.69 KB & 3.52 KB & 2.34 KB & 1.17 KB \\\hline
	\end{tabular}
	
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for the Iris flower and PCA algorithm.}
\end{table}

Kruskal's stress kept bellow 11\% for all reductions. Indeed, variations in the learning phase performance were small: 1-NN and SVM score were high along all reductions (except for the last one, perhaps, where 1-NN decreased to .87).

\paragraph{Reducing the Iris Flower Data Set With the ISOMAP Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/iso_iris}
	\caption{The reductions of Iris Flower to 3, 2, and 1 dimensions, respectively, using the ISOMAP algorithm.}
	\label{fig:dsirisiso}
\end{figure}

It is clear, from figure \ref{fig:dsirisiso}, that the ISOMAP algorithm has also resulted in acceptable reductions of the Iris Flower data set, where classes kept a similar organization compared to the reductions made by the PCA algorithm.

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.25\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^3$ & $\mathbb{R}^2$ & $\mathbb{R}$ \\\hline
		\textbf{1-NN} & 1 & 1 & 1 & .87 \\\hline
		\textbf{Pred. accuracy} & .99 & .98 & .98 & .93 \\\hline
		\textbf{Best parameters} & k: rbf, g: 0.01, C: 100 & k: linear, C: 10 & k: linear, C: 10 & k: sigmoid, g: 0.1, C: 1 \\\hline
		\textbf{GridSearch time} & 2 s & 2.55 s & 2.42 s & 2.37 s \\\hline
		\textbf{Reduction time} & - & 1.28 s & 1.3 s & 1.27 s \\\hline
		\textbf{Kruskal's stress} & - & .15 & .15 & .16 \\\hline
		\textbf{Data size} & 4.69 KB & 3.52 KB & 2.34 KB & 1.17 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for the Iris flower and ISOMAP algorithm.}
\end{table}

The results were very similar to the ones obtained when applying the PCA algorithm. The differences consist on the Kruskal's stress, now limited to 16\% and the reduction time, which has consistently increased due to the higher computational cost of the ISOMAP algorithm.

\clearpage
\section{The Swiss Roll Data Set}
\label{experiments:sr}

The Swiss Roll data set, with 1000 samples and 3 features.
\newline\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{datasets/swiss}
	\captionsetup{justification=centering}
	\caption{The Swiss Roll data set.}
\end{figure}

\newpage
\paragraph{Reducing the Swiss Roll Data Set With the PCA Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/pca_swiss}
	\caption{The reductions of the Swiss Roll to 3, 2 and 1 dimensions, respectively, with the PCA algorithm.}
	\label{fig:dsswisspca}
\end{figure}

Figure \ref{fig:dsswisspca} illustrates how the PCA algorithm is not suitable for reducing the Swiss-roll, given its non-linear distribution. Reductions to 2 and 1 dimensions have made very dissimilar samples to become mixed, damaging the original structure of the data set and, of course, confusing learners and reducing their score in the learning process:

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.20\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^3$ & $\mathbb{R}^2$ & $\mathbb{R}$ \\\hline
		\textbf{1-NN} & 1 & 1 & .26 & .2 \\\hline
		\textbf{Score} & 1 & 1 & .68 & .54 \\\hline
		\textbf{Best parameters} & k: rbf, C: 100, g: 0.01 & C: 100, k: rbf, g: 0.01 & C: 100, k: rbf, g: 0.1 & C: 1, k: rbf, g: 0.1\\\hline
		\textbf{GridSearch time} & 12.72 s & 15.13 s & 9.07 s & 7.11 s \\\hline
		\textbf{Reduction time} & - & 2.15 s & 2.02 s & 2.08 s\\\hline
		\textbf{Kruskal's stress} & - & 0 & .28 & .53 \\\hline
		\textbf{Data size} & 23.44 KB & 23.44 KB & 15.63 KB & 7.81 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for the Swiss Roll and PCA algorithm.}
\end{table}

Notice that the learners trained during the grid search can achieve high scores in the original space $\mathbb{R}^3$. When the same search is applied to the data set reductions, learners only manage to retrieve significantly lower scores. Clearly, reducing nonlinear data sets with linear methods has a highly negative impact on the learning process.

\paragraph{Reducing the Swiss Roll Data Set With the ISOMAP Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/iso_swiss}
	\caption{The reductions of the Swiss Roll to 3, 2 and 1 dimensions, respectively, with the ISOMAP algorithm.}
	\label{fig:dsswissiso}
\end{figure}

Figure \ref{fig:dsswissiso} illustrates the efficacy of the ISOMAP algorithm in reducing the Swiss Roll while still preserving the original structure of the data set (similar samples were maintained close to each other). Of course, this has a positive impact on learning:

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.20\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^3$ & $\mathbb{R}^2$ & $\mathbb{R}$ \\\hline
		\textbf{1-NN} & 1 & 1 & 1 & 1 \\\hline
		\textbf{Score} & 1 & 1 & 1 & 1 \\\hline
		\textbf{Best parameters} & k: rbf, C: 100, g: 0.01 & C: 10, k: rbf, g: 0.01 & C: 100, k: rbf, g: 0.01 & C: 10, k: rbf, g: 0.01 \\\hline
		\textbf{GridSearch time} & 2.41 s & 2.21 s & 2.13 s & 2.35 s \\\hline
		\textbf{Reduction time} & - & 16.32 s & 16.75 s & 14.72 s\\\hline
		\textbf{Kruskal's stress} & - & .49 & .37 & .46 \\\hline
		\textbf{Data size} & 23.44 KB & 23.44 KB & 15.63 KB & 7.81 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for the Swiss Roll and ISOMAP algorithm.}
\end{table}

Differently from the reductions with the PCA algorithm, learners trained over reductions retrieved from ISOMAP maintained a perfect score throughout the entire experiment, confirming that ISOMAP is a more adequate method for nonlinear data set reduction.

\clearpage
\section{The Digits Data Set}

Digits data set is composed by 1797 samples, 64 features and 10 classes. Each sample is a 8x8 image of a hand-written digit from 0 to 9.
\newline\newline

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{datasets/digits}
	\caption{The Digits data set.}
\end{figure}

\newpage
\paragraph{Reducing the Digits Data Set With the PCA Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/pca_digits}
	\caption{The reductions of Digits to 3, 2 and 1 dimensions, respectively, with the PCA algorithm.}
	\label{fig:dsdigitspca}
\end{figure}

From figure \ref{fig:dsdigitspca}, we have glimpse of Digits's samples structure. The many classes' clusters are visible in the 3D, although they become highly mixed in the 2D and 1D, yielding a confusing representation.

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^{10}$ & $\mathbb{R}^3$ & $\mathbb{R}^2$ & $\mathbb{R}$ \\\hline
		\textbf{1-NN} & .99 & .97 & .68 & .56 & .28 \\\hline
		\textbf{Score} & .98 & .95 & .74 & .64 & .39  \\\hline
		\textbf{Best parameters} & k: rbf, C: 10, g: .001 & k: rbf, C: 10, g: .001 & k: rbf, C: 10, g: .01 & k: rbf, C: 1, g: .01 & k: rbf, C: 10, g: .001\\\hline
		\textbf{GridSearch time} & 11.75 s & 26.97 s & 150.91 s & 125.64 s & 104.41 s \\\hline
		\textbf{Reduction time} & - & 6.02 s & 6.54 s & 6.05 s & 6.1 s \\\hline
		\textbf{Kruskal's stress} & - & .16 & .42 & .54 & .71 \\\hline
		\textbf{Data size} & 898.5 KB & 140.4 KB & 42.1 KB & 28.08 KB & 14 KB \\\hline
	\end{tabular}
	
	\caption{Description of predictions and reduction performance for Digits and ISOMAP algorithm.}
\end{table}

Notice that it was possible to eliminate 54 dimensions, consistently reducing the data set size, and only suffering 3\% of prediction accuracy loss. The score drastically decreased, however, when more dimensions were removed. Furthermore, reductions to less than 10 dimensions would generate highly mixed samples, requiring more time for grid searching. Finally, it is also clear that score was inversely proportional to stress.

\paragraph{Reducing the Digits Data Set With the ISOMAP Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/iso_digits}
	\caption{The reductions of Digits to 3, 2 and 1 dimensions, respectively, with the ISOMAP algorithm.}
	\label{fig:dsdigitsiso}
\end{figure}

Digits reductions using ISOMAP, as illustrated in figure \ref{fig:dsdigitsiso}, have proven to be much more organized, when compared to the reductions made by the PCA algorithm. Indeed, the representation in the 2D shows much less mixed samples, which translates into better scores:

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|p{.14\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^{10}$ & $\mathbb{R}^3$ & $\mathbb{R}^2$ & $\mathbb{R}$ \\\hline
		\textbf{1-NN} & .99 & .99 & .97 & .92 & .46 \\\hline
		\textbf{Score} & .94 & .96 & .93 & .9 & .56 \\\hline
		\textbf{Best parameters} & k: linear, C: 1 & k: linear, C: 1 & k: linear, C: 1 & k: linear, C: 1 & g: 0.01, k: rbf, C: 100 \\\hline
		\textbf{GridSearch time} & 7.68 s & 5.1 s & 4.59 s & 4.47 s & 5.44 s \\\hline
		\textbf{Reduction time} & - & 47.93 s & 47.99 s & 48.31 s & 48.82 s \\\hline
		\textbf{Kruskal's stress} & - & .16 & .44 & .48 & .63 \\\hline
		\textbf{Data size} & 898.5 KB & 140.4 KB & 42.1 KB & 28.08 KB & 14 KB \\\hline
	\end{tabular}
	
	\caption{Description of predictions and reduction performance for Digits and ISOMAP algorithm.}
\end{table}

1-NN's and classification accuracy were kept above 90\% for all reductions down to the $\mathbb{R}^2$, being considerably higher than the scores obtained when testing over Digits reduced with PCA. We can compare the reduction time as well, which has increased significantly given the data set great number of samples and the higher complexity of ISOMAP compared to PCA.

The Grid Search performed over the reduction to the $\mathbb{R}^{10}$ only took 58.2\% of the time of this same search when performed over the original data set: a consequence of the great reduction of features, no doubt, filtering the necessary work done by estimators in the learning phase.

\clearpage
\section{The Glass Data Set}

Motivated by criminological investigations, the classification of different types of glasses based on the material composition can be demonstrated by the Glass data set, which contains 214 samples and 10 features, where each sample belongs to a specific class (e.g., building windows glass, tableware and headlamps). Lastly, it must be remarked that the first feature is the identification number of a given sample, and was therefore removed during our experiment.
\newline\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{datasets/glass}
	\captionsetup{justification=centering}
	\caption{The Glass data set.}
\end{figure}

\newpage
\paragraph{Reducing the Glass Data Set With the ISOMAP Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/iso_glass}
	\caption{The reductions of Glass to 3, 2 and 1 dimensions, respectively, with the ISOMAP algorithm.}
	\label{fig:dsglassiso}
\end{figure}

We can observe from the representation of Glass in the $\mathbb{R}^3$ (figure \ref{fig:dsglassiso}) that samples from different classes present little separation from each other. Indeed, learners will have more difficulty when processing such reductions.

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.15\linewidth}|p{.12\linewidth}|p{.12\linewidth}|p{.12\linewidth}|p{.12\linewidth}|p{.12\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^8$ & $\mathbb{R}^6$ & $\mathbb{R}^4$ & $\mathbb{R}^3$ \\\hline
		\textbf{1-NN} & .63 & .63 & .6 & .63 & .65 \\\hline
		\textbf{Score} & .64 & .65 & .63 & .65 & .65 \\\hline
		\textbf{Best parameters} & k: rbf, g: 0.001, C: 1000 & k: rbf, g: 0.01, C: 100 & k: rbf, g: 0.1, C: 100 & k: rbf, g: 0.1, C: 10 & k: rbf, g: 0.1, C: 10\\\hline
		\textbf{GridSearch time} & 1.96 s & 2.85 s & 2.89 s & 3.21 s & 3.40 s \\\hline
		\textbf{Reduction time} & - & 2.8 s & 2.96 s & 2.87 s & 2.90 s \\\hline
		\textbf{Kruskal's stress} & - & .2 & .16 & .15 & .21 \\\hline
		\textbf{Data size} & 15.05 KB & 13.38 KB & 10.03 KB & 6.69 KB & 5.02 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for Glass and ISOMAP algorithm.}
\end{table}

ISOMAP was able to reduce Glass with little structure deterioration, where 1-NN's and prediction accuracy were maintained throughout the entire experiment, while no complexity increase in the learner parameters is visible. Grid Search time slightly increased as the linear SVM requires more time given high values of C and the compaction of data, which crunched samples together.

\clearpage
\section{The Dermatology Data Set}

The Dermatology data set, composed by 366 samples and 35 features. Here, the target is to correctly diagnose the erythemato-squamous diseases.
\newline\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{datasets/dermatology}
	\captionsetup{justification=centering}
	\caption{The Dermatology data set.}
\end{figure}

\newpage
\paragraph{Reducing the Dermatology Data Set With the ISOMAP Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/iso_dermatology}
	\caption{The reductions of Dermatology to 3, 2 and 1 dimensions, respectively, with the ISOMAP algorithm.}
	\label{fig:dsdermatologyiso}
\end{figure}

From figure \ref{fig:dsdermatologyiso}, we easily infer that two classes are trivially separable. The rest, however, is highly convoluted in the origin of the space, hence higher dimensions could be necessary to achieve high accuracy scores.

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.15\linewidth}|p{.12\linewidth}|p{.12\linewidth}|p{.12\linewidth}|p{.12\linewidth}|p{.12\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^{20}$ & $\mathbb{R}^{10}$ & $\mathbb{R}^3$ & $\mathbb{R}^2$ \\\hline
		\textbf{1-NN} & .91 & .89 & .91 & .72 & .74 \\\hline
		\textbf{Score} & .95 & .95 & .96 & .8 & .78 \\\hline
		\textbf{Best parameters} & k: linear, C: 10 & k: linear, C: 1 & k: linear, C: 1 & k: linear, C: 100 & k: linear, C: 1\\\hline
		\textbf{GridSearch time} & 2.76 s & 2.61 s & 2.57 s & 14.47 s & 25.66 s \\\hline
		\textbf{Reduction time} & - & 66.04 s & 65.89 s & 65.85 s & 66.18 s \\\hline
		\textbf{Kruskal's stress} & - & .02 & .03 & .07 & .1 \\\hline
		\textbf{Data size} & 97.22 KB & 57.19 KB & 28.59 KB & 8.58 KB & 5.72 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for Glass and ISOMAP algorithm.}
\end{table}

Indeed, tests over reductions to the $\mathbb{R}^3$ and $\mathbb{R}^2$ presented lower scores when compared to the ones performed over higher dimensions such as $\mathbb{R}^{20}$ and $\mathbb{R}^{10}$. These latter presented very similar scores to the ones achieved when testing over the original dimensional space, entailing that no drastic structural changes were made.

\clearpage
\section{The Leukemia Data Set}

The Leukemia data set contains 72 samples and 7129 features, which express levels of the genes in a given patient. Each sample belongs to a class $t \in \{-1, 1\}$, tagging which of two variants of leukemia is present in the sample (AML, 25 samples, or ALL, 47 samples) \cite{on:duc_ds}.
\newline\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{img/datasets/leukemia}
	\captionsetup{justification=centering}
	\caption{The Leukemia data set.}
\end{figure}

\newpage
\paragraph{Reducing the Leukemia Data Set With the ISOMAP Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/iso_leukemia}
	\caption{The reductions of Leukemia to 3, 2 and 1 dimensions, respectively, with the ISOMAP algorithm.}
	\label{fig:dsleukemiaiso}
\end{figure}

Visualization has barely changed from 7129 to 3 dimensions, as samples are still very convoluted.

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^{30}$ & $\mathbb{R}^{20}$ & $\mathbb{R}^{10}$ \\\hline
		\textbf{1-NN} & .6 & .4 & .33 & .47\\\hline
		\textbf{Score} & .99 & .88 & .9 & .89 \\\hline
		\textbf{Best parameters} & coef0: 10, degree: 2, k: poly & C: 1, k: linear & C: 1, k: linear & C: 1, k: linear \\\hline
		\textbf{GridSearch time} & 4.66 s & 2.56 s & 2.48 s & 2.53 s \\\hline
		\textbf{Reduction time} & - & .1 s & .1 s & .09 s \\\hline
		\textbf{Kruskal's stress} & - & 1.09 & .97 & .67 \\\hline
		\textbf{Data size} & 4010.06 KB & 16.88 KB & 11.25 KB & 5.62 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for Leukemia and ISOMAP algorithm.}
\end{table}

An impressive reduction was performed, where a data set lying on the $\mathbb{R}^{7129}$ and occupying 4010.06 KB was shrunk to the $\mathbb{R}^{30}$, now only occupying 16.88 KB.

We observe very high values for the Kruskal's stress and low values for the 1-NN classifier, which suggests that tests with the SVM would also achieve low prediction accuracy. This, however, did not happen, as scores maintained a baseline of 88\% for all reductions. We therefore take this experiment as an example of how Kruskal's low stress is not always aligned with learners good performance and might not be a suitable metric for evaluation.

Finally, as many features were discarded, Grid Search performed faster when applied over the reductions than when performed over the original data set.
\clearpage

\section{The WDBC Data Set}

The Wisconsin Diagnostic Breast Cancer (WDBC) data set, containing 596 samples and 32 features computed from a breast mass. WDBC has 357 benign samples and 212 malignant. During this experiment, the first feature was disregarded, as it represents the identification numbers of the samples.
\newline\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{img/datasets/wdbc}
	\captionsetup{justification=centering}
	\caption{The WDBC data set.}
	\label{fig:dswdbc}
\end{figure}

\newpage
\paragraph{Reducing the WDBC Data Set With the ISOMAP Algorithm}

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{experiments/iso_wdbc}
	\caption{The reductions of WDBC to 3, 2 and 1 dimensions, respectively, with the ISOMAP algorithm.}
	\label{fig:dswdbciso}
\end{figure}

Reductions over WDBC seem to have somewhat contorted the original structure and and do not help significantly in visualization.

\begin{table}[H]
	\centering
	
	\begin{tabular}{|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|p{.15\linewidth}|}
		\hline
		& \textbf{Original} & $\mathbb{R}^{20}$ & $\mathbb{R}^{10}$ & $\mathbb{R}^{3}$ \\\hline
		\textbf{1-NN} & .91 & .89 & .47 & .89 \\\hline
		\textbf{Score} & .95 & .89 & .9 & .9 \\\hline
		\textbf{Best parameters} & C: 1, k: linear & C: 10, k: linear & C: 10, k: linear & C: 10, k: linear \\\hline
		\textbf{GridSearch time} & 18.54 s & 83.01 s & 78.96 s & 61.51 s \\\hline
		\textbf{Reduction time} & - & 3.23 s & 3.22 s & 3.3 s \\\hline
		\textbf{Kruskal's stress} & - & .55 & .47 & .38 \\\hline
		\textbf{Data size} & 133.36 KB & 88.91 KB & 44.45 KB & 13.34 KB \\\hline
	\end{tabular}
	\captionsetup{justification=centering}
	\caption{Description of predictions and reduction performance for WDBC and ISOMAP algorithm.}
\end{table}

Although prediction accuracy was kept high throughout the whole experiment, Grid Search time has intensively increased, which indicates that the estimators would be having a great difficulty when learning from reductions. We conclude that ISOMAP might not be the ideal option to reduce WDBC for the dimensions specified in the table.
