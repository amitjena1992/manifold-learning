\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{ren2010}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Relevant Background}{2}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data set}{2}{section.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Example of a canonical data set}{2}{subsubsection*.4}}
\newlabel{irisdataset}{{2.1}{2}{Example of a canonical data set}{subsubsection*.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The first three samples of the Iris flower data set.\relax }}{3}{table.caption.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Data set as a collection of vectors in the $\mathbb  {R}^n$}{3}{subsection.2.1.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The first three samples of the Indian Liver Patient Dataset (ILPD)\relax }}{3}{table.caption.6}}
\citation{cay2005}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The data set ILPD mapped onto $\mathbb  {R}^n$, where each of its features is an axis, except for $S \mathrel {\mathrel {\mathop :}\mkern -1.2mu=}\{1, 2\}$, which was represented by the vertices' colors.\relax }}{4}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:disp_ilpd}{{2.1}{4}{The data set ILPD mapped onto $\mathbb {R}^n$, where each of its features is an axis, except for $S \coloneqq \{1, 2\}$, which was represented by the vertices' colors.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Modern Problems and Applications}{4}{subsection.2.1.2}}
\citation{gho2006}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces A data set with 8200 samples and 100 features.\relax }}{5}{table.caption.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Probability Theory}{5}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Feature Standardization}{5}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Centering Matrix}{5}{subsection.2.2.2}}
\citation{ross2010introductory}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Variance}{6}{subsection.2.2.3}}
\citation{ross2010introductory}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Covariance}{7}{subsection.2.2.4}}
\@writefile{toc}{\contentsline {paragraph}{Covariance Matrix of Features in a Centered Data Set}{7}{paragraph*.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Correlation}{8}{subsection.2.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{Correlation Matrix of Features in a Centered Data Set}{8}{subsubsection*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Numerical Analysis}{8}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Eigenvalues and Eigenvectors of a Matrix}{8}{subsection.2.3.1}}
\citation{cox2001}
\citation{gan2008}
\citation{lee2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Spectral Decomposition of a Matrix}{9}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Singular Value Decomposition}{9}{subsection.2.3.3}}
\newlabel{sec:svd}{{2.3.3}{9}{Singular Value Decomposition}{subsection.2.3.3}{}}
\newlabel{th:svd-aat}{{2.3.1}{9}{Singular Value Decomposition}{theorem.2.3.1}{}}
\citation{lee2009}
\citation{lee2002}
\citation{lee2002}
\citation{stereo_proj}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Topology}{10}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Manifolds}{10}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{Charts}{10}{subsubsection*.11}}
\@writefile{toc}{\contentsline {subsubsection}{Atlas}{10}{subsubsection*.12}}
\citation{berge1973}
\citation{berge1973}
\citation{may1972}
\citation{berge1973}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Stereographic projection applied to Earth.\relax }}{11}{figure.caption.13}}
\newlabel{fig:stereographic_earth}{{2.2}{11}{Stereographic projection applied to Earth.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Graph Theory}{11}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Graphs}{11}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{Basic Concepts \cite  {berge1973}}{11}{subsubsection*.14}}
\@writefile{toc}{\contentsline {subsubsection}{Further Specifications}{12}{subsubsection*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The \textbf  {Les Miserables} graph.\relax }}{12}{figure.caption.16}}
\newlabel{fig:graph}{{2.3}{12}{The \textbf {Les Miserables} graph.\relax }{figure.caption.16}{}}
\citation{berge1973}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A tree extracted (a subgraph) from the \textbf  {Les Miserables} graph.\relax }}{13}{figure.caption.17}}
\newlabel{tree}{{2.4}{13}{A tree extracted (a subgraph) from the \textbf {Les Miserables} graph.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Related Problems}{13}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Nearest-Neighbor Search}{13}{subsubsection*.18}}
\@writefile{toc}{\contentsline {paragraph}{A Generic Algorithm for NN Search}{13}{paragraph*.19}}
\citation{cor2011}
\newlabel{fig:example-graph}{{2.5a}{14}{$G$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:example-graph}{{a}{14}{$G$\relax }{figure.caption.20}{}}
\newlabel{fig:example-graph-nn}{{2.5b}{14}{$H$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:example-graph-nn}{{b}{14}{$H$\relax }{figure.caption.20}{}}
\newlabel{fig:example-graph-en}{{2.5c}{14}{$I$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:example-graph-en}{{c}{14}{$I$\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Single-pair Shortest-path Problem \cite  {cor2011}}{14}{subsubsection*.21}}
\citation{cor2011}
\@writefile{toc}{\contentsline {paragraph}{Dijkstra's Algorithm \cite  {cor2011}}{15}{paragraph*.22}}
\citation{golin2003floydwarshall}
\newlabel{fig:spa-graph}{{2.6a}{16}{The weighted graph $G$.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:spa-graph}{{a}{16}{The weighted graph $G$.\relax }{figure.caption.23}{}}
\newlabel{fig:spa-tree}{{2.6b}{16}{The shortest-path tree $S$.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:spa-tree}{{b}{16}{The shortest-path tree $S$.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{All-pairs Shortest-path Problem}{16}{subsubsection*.24}}
\@writefile{toc}{\contentsline {paragraph}{Floyd-Warshall Algorithm \cite  {golin2003floydwarshall}}{16}{paragraph*.25}}
\citation{pat1996}
\citation{hot2009}
\citation{pat1996}
\citation{awa2015}
\citation{awa2015}
\citation{zhu2009}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Machine Learning}{17}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Machine Learning Algorithms}{17}{subsection.2.6.1}}
\citation{awa2015}
\citation{sksvm}
\citation{sksvm}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Support Vector Machine: An Example of Supervised Learning}{18}{subsection.2.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces A SVM classifier projecting a hyperplane that perfectly separates two classes of samples. \cite  {sksvm}\relax }}{18}{figure.caption.26}}
\newlabel{fig:svmmargin}{{2.7}{18}{A SVM classifier projecting a hyperplane that perfectly separates two classes of samples. \cite {sksvm}\relax }{figure.caption.26}{}}
\citation{wessvmdef}
\citation{wessvmdef}
\newlabel{svmconst}{{2.1}{19}{Support Vector Machine: An Example of Supervised Learning}{equation.2.6.1}{}}
\newlabel{eq:eqsvmwidth}{{2.2}{19}{Support Vector Machine: An Example of Supervised Learning}{equation.2.6.2}{}}
\newlabel{eq:svmminw}{{2.3}{19}{Support Vector Machine: An Example of Supervised Learning}{equation.2.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{SVM for non-separable data sets (soft margins)}{19}{subsubsection*.27}}
\citation{mitsvm}
\citation{mitsvm}
\citation{mitsvm}
\@writefile{toc}{\contentsline {subsubsection}{Dependency over the dot product}{20}{subsubsection*.28}}
\newlabel{eq:svmlag1}{{2.4}{20}{Dependency over the dot product}{equation.2.6.4}{}}
\newlabel{svmlag2}{{2.5}{20}{Dependency over the dot product}{equation.2.6.5}{}}
\newlabel{svmlag3}{{2.6}{20}{Dependency over the dot product}{equation.2.6.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Kernel functions}{20}{subsubsection*.29}}
\citation{svmkernels}
\citation{svmkernels}
\citation{svmkernels}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Projection of samples from the $\mathbb  {R}$ to the $\mathbb  {R}^2$, allowing SVM to find a hyperplane that perfectly separates both classes. \cite  {svmkernels}\relax }}{21}{figure.caption.30}}
\newlabel{fig:svmkernel}{{2.8}{21}{Projection of samples from the $\mathbb {R}$ to the $\mathbb {R}^2$, allowing SVM to find a hyperplane that perfectly separates both classes. \cite {svmkernels}\relax }{figure.caption.30}{}}
\citation{rif2008}
\citation{ovacj}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Multi-class Classification}{22}{subsection.2.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Evaluating learners}{23}{subsection.2.6.4}}
\@writefile{toc}{\contentsline {subsubsection}{Confusion Matrix}{23}{subsubsection*.31}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Example of confusion matrix for a data-set with four different classes.\relax }}{23}{table.caption.32}}
\citation{crossvalid}
\citation{gridsearch}
\citation{roh2015}
\citation{roh2015}
\citation{roh2015}
\@writefile{toc}{\contentsline {subsubsection}{Cross Validation}{24}{subsubsection*.33}}
\@writefile{toc}{\contentsline {subsubsection}{Grid Search}{24}{subsubsection*.34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Examples of Learning}{24}{subsection.2.6.5}}
\@writefile{toc}{\contentsline {subsubsection}{Coffee Selling Rate}{24}{subsubsection*.35}}
\citation{roh2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces graphic representation of a data set generalization by a linear (orange) and a nonlinear model (green). \cite  {roh2015}\relax }}{25}{figure.caption.36}}
\newlabel{fig:rohrer2015}{{2.9}{25}{graphic representation of a data set generalization by a linear (orange) and a nonlinear model (green). \cite {roh2015}\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Iris Flower}{25}{subsubsection*.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Confusion matrix of a SVM with $C=100$, $gamma=.01$ and $rbf$ kernel when predicting samples from the Iris flower data set.\relax }}{26}{figure.caption.38}}
\newlabel{fig:cmsvmiris}{{2.10}{26}{Confusion matrix of a SVM with $C=100$, $gamma=.01$ and $rbf$ kernel when predicting samples from the Iris flower data set.\relax }{figure.caption.38}{}}
\@setckpt{chapters/background}{
\setcounter{page}{27}
\setcounter{equation}{6}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{6}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{4}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{26}
\setcounter{float@type}{8}
\setcounter{lstnumber}{5}
\setcounter{theorem}{0}
\setcounter{remark}{1}
\setcounter{example}{0}
\setcounter{experiment}{0}
\setcounter{section@level}{0}
\setcounter{lstlisting}{0}
}
