\chapter{Nonlinear Dimensionality Reduction}

Although PCA presents promising results in many cases, hence its great popularity in dimensionality reduction problems, there are many examples in which PCA will fail to understand the structure of the data. Consider the example bellow:

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	
	\includegraphics[width=.5\linewidth]{datasets/swiss}
	\caption{The Swiss roll data set.}
	\label{fig:dsswiss}
\end{figure}

The application of the PCA algorithm over the Swiss Roll data set results in the following reductions:

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}

	\includegraphics[width=\linewidth]{experiments/pca_swiss}
	\caption{The Swiss Roll data set reductions to 3, 2 and 1 dimensions, respectively, using the PCA algorithm.}
	\label{fig:swiss_pca}
\end{figure}

Looking at figure \ref{fig:swiss_pca} (specially in the last reduction, to a single dimension), it becomes clear that PCA has a big draw back: it assumes that the data lies on a liner subspace \cite{cay2005} and, therefore, applying linear transformations to it will rotate and scale it, without distort the original data structure. When this assumption does not hold, PCA will incorrectly extract the underlying structure, possibly mixing very dissimilar samples.

Clearly, linear dimensionality reduction techniques are not adequate to reduce the Swiss-roll. In fact, it is not adequate to reduce any data set lying on a nonlinear manifold.

\section{The ISOMAP Algorithm}

Firstly suggested by Tenenbaum, de Silva and Langfor, \textbf{Isometric Feature Mapping} (or ISOMAP) assumes that the data lies near a smooth manifold. If the assumption is reasonable, it is possible to explore concepts such as neighborhood and local linearity to map the manifold to a linear structure before reducing it with a linear algorithm.

\subsection{Study of the ISOMAP Algorithm}
\label{sec:studying_ISOMAP}

As the original data set might be folded, twisted or curved \cite{ten2000}, we must first find a suitable linear representation for it.

Let $\vect S$ be our original data set, as illustrated in figure \ref{fig:mani_s}.

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=.5\linewidth]{datasets/s}
	\caption{The data set $\vect S$, consisting of $1000$ samples and $3$ features.}
	\label{fig:mani_s}
\end{figure}

Additionally, consider the symmetric undirected weighted graph $G=(V, E)$ and $w \colon E \to \mathbb{R} \mid w(x, y) = \delta_{xy}$, where $\delta_{xy}$ is the euclidean distance between the samples $\vect x$ and $\vect y$ in $\vect S$. That is,
$$\delta_{xy} = \sqrt{\sum_{i} (x_i - y_i)^2}, \forall (\vect x, \vect y) \in \vect S \times \vect S \mid \vect x \ne \vect y$$

Now that only distances were kept, an infinite number of $n$-dimensional embeddings can be found with \textbf{MDS}, as every reduction can be transposed, rotated or reflected while maintaining the original dissimilarities. This does not handle the non-linearity of the data, though, as the original distances strictly constraint the samples to their original pattern. In order to ``unfold" the data set, \textbf{Nearest neighbor search} can be performed over $G$, resulting in the graph $G'$. Nearest-neighbor search will preserve edges connecting closer samples, hence preserving local (linear) distances, but erase edges connecting samples which are far from each other (non necessarily linear). Obviously, the search parameters ($k$ or $\epsilon$) must be carefully chosen to limit the connectivity of the vertices to a small (linear) neighborhood while maintaining the graph completely connected.

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=.5\linewidth]{studying_isomap/neighbors}
	\caption{The graph $G'$.}
	\label{fig:mani_s_graph_h}
\end{figure}

At this moment, not all distances in $G'$ are defined. This can be easily handled, though, by performing the \textbf{Floyd-Warshall} algorithm over $G'$, resulting in the shortest-paths graph $G''$. Alternatively, $G''$ can be achieved by performing \textbf{Dijkstra's algorithm} for all nodes and joining all shortest-path trees found.

Finally, $G''$ is a euclidean graph which roughly lies on a linear subspace. Furthermore, the adjacency matrix associated to $G''$ contains not the distance induced by the $L_2$ norm, but the geodesic pairwise distances \cite{gho2006}. The \textbf{MDS} method can now be used to construct a representation in sub-spaces of the $\mathbb{R}^n$. $\vect S$, specifically, can be reduced to the $\mathbb{R}^2$ or $\mathbb{R}$:

\begin{figure}[H]
    \begin{subfigure}{.5\linewidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=\linewidth]{studying_isomap/embedding_2}
		\caption{Reduction to 2D.}
		\label{fig:mani_s_reduced_1}
	\end{subfigure}%
	\begin{subfigure}{.5\linewidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=.63\linewidth]{studying_isomap/embedding_1}
		\caption{Reduction to 1D.}
		\label{fig:mani_s_reduced_2}
	\end{subfigure}
	\caption{$\vect S$' reductions.}
	\label{fig:mani_s_reduced}
\end{figure}

\newpage
\subsection{Formalization of the ISOMAP Algorithm}

Let $\vect X$ be the original data set and $p \in \mathbb{R}$ the number of dimensions desired for the reduced data set \cite{ten2000},

\begin{enumerate}
	\item Construct the weighted graph $G$ from the distances pairwise $\delta_{xy}, \forall (\vect x, \vect y) \in \vect X \times \vect X, \vect x \ne \vect y$ and find the graph $G'$ by applying the \textbf{nearest-neighbor algorithm} on the graph $G$.

	\item Compute the shortest path graph $G''$ between all pairs of nodes from graph $G'$. This might be done by the \textbf{all-pairs Dijkstra's} or by the \textbf{Floyd-Warshall} algorithm.

	\item Use $G''$ to construct the $p$-dimensional embedding using the \textbf{MDS} algorithm.
\end{enumerate}

\subsection{Computational Complexity}
\label{sec:isocomplexity}

If $n$ is the number of training samples, $f \in \mathbb{R}$ the number of features and $k \in \mathbb{R}$ the number of nearest neighbors:

\begin{enumerate}
	\item The time complexity associated with building the neighborhood graph is $O(n^2)$, whereas the space necessary to represent the distances kept is $n^2$.
	\item Algorithms for finding shortest path graph can be executed in-place and do not increase space complexity. Regarding time complexity, however:
	\begin{enumerate}
		\item Dijkstra's algorithm implementation using Fibonacci Heaps have time complexity $O(nk + n \log n)$. As the algorithm must be calculated for each node, this step has time complexity $O[n^2(k + \log n)]$.
		\item Alternatively, using the Floyd-Warshall algorithm, this step has time complexity equals to $O(n^3)$.
	\end{enumerate}
	\item Taking $O(n^3)$ time steps to calculate the spectral decomposition, MDS clearly is the bottleneck of the entire algorithm \cite{cay2005, silva2002global}. As for space complexity, $O(f^2 + nf)$ is required for storing the matrices $\vect \Sigma$ and $\vect U$.
\end{enumerate}

The time complexity of the ISOMAP algorithm (when using Dijkstra's) is, therefore, $O[n^2 + n^2(k + \log n) + n^3]$, and the space complexity is $O(n^2 + f^2 + nf)$.

\begin{experiment}[Timing the ISOMAP Algorithm]
	Consider the data set \textbf{Digits} with 1797 samples and 64 features. The following table indicates the time duration for each step of the ISOMAP algorithm when reducing Digits to 3 dimensions:

	\begin{table}[H]
		\centering
		\begin{tabular}{|l|r|}
			\hline
			\textbf{Pairwise distances from data set} & .44 s \\\hline
			\textbf{K Nearest Neighbors Search} & 1.3 s \\\hline
			\textbf{All Pairs Dijkstra's} & 51.44 s\\\hline
			\textbf{MDS} & 118.84 s \\\hline
			\textbf{Total Time} & 172.69 s \\\hline
		\end{tabular}

		\caption{Listing of time spent on each step of the ISOMAP algorithm.}
	\end{table}

	As expected, most of the time (68.81\%) was spent executing MDS.
\end{experiment}

In practice, ISOMAP's time complexity of $O[n^2 + n^2(k + \log n) + n^3]$ makes it unsuitable for data sets with high number of samples. This is illustrated in Shi and Gu's experiments: ISOMAP could not reduce data sets with more than 6000 samples in reasonable time \cite{shi2012fast}.

\begin{experiment}
	Let \textbf{Spam} be a data set with 4601 samples and 57 features. The table bellow describes the time necessary for the implemented ISOMAP and sk-ISOMAP (the algorithm from the \href{http://scikit-learn.org}{scikit-learn} library) to reduce Spam to 3 dimensions:

	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Algorithm} & Time \\\hline
			\textbf{ISOMAP} & 29.9 m \\\hline
			\textbf{sk-ISOMAP} & 10.76 s \\\hline
		\end{tabular}

		\caption{Timing the implemented ISOMAP and scikit-learn's implementation.}
	\end{table}

	The original ISOMAP implemented required almost 30 minutes to reduce Spam.
	Surprisingly, scikit-learn's implementation executed incredibly fast. A set of factors contribute for this result:

	\begin{itemize}
		\item \textbf{Ball Tree} is used for efficient neighbor search (requiring only $O(f n \log k \log n)$ time steps).
		\item Many components in scikit-learn have python signatures, but are actually implemented in C, considerably increasing performance.
		\item ISOMAP is implemented as a kernel for the \textbf{KernelPCA} method. Further investigation is done in section \ref{sec:ISOMAP_extensions}.
	\end{itemize}

\end{experiment}

\subsection{Extensions}
\label{sec:ISOMAP_extensions}

\subsubsection{ISOMAP as a variation of Kernel PCA}

While ``classic" PCA aims to find the principal components of the covariance matrix $\vect \Sigma = \frac{1}{n} \vect X^\top \vect X = \frac{1}{n} \sum_k x_{ik} \cdot x_{kj}, \forall (i, j) \in [0, |\vect S|]^2$ of a centered data set $\vect X$, it can be easily modified to attempt to do the same, but over the matrix $\vect \Sigma' = \frac{1}{n} \sum_i \phi(x)_i \cdot \phi(y)_i = \frac{1}{n} \sum_i K(x, y)$, where $\phi \colon \mathbb{R}^f \to \mathbb{R}^p$ is a function that projects vectors in the original $f$-space to a different $p$-space and $K \colon \mathbb{R}^f \times \mathbb{R}^f \to \mathbb{R}$ is a kernel function. Such modification is named Kernel PCA.

In his paper, Ghodsi describes how MDS (and therefore, ISOMAP) is equivalent to Kernel PCA, given it is using the following kernel function \cite{gho2006}:

\begin{align*}
\vect K_{ISOMAP} = -\frac{1}{2}\vect{H \delta^2 H}
\end{align*}

Where $\vect{H \delta^2 H}$ is the double centered squared dissimilarity matrix, i.e, the double centered squared geodesic distances between samples found from the shortest-path graph.

An advantage of substituting the MDS algorithm by Kernel PCA is that the latter provides an embedding, i.e., a transformation matrix from the original space to the reduced one; whereas MDS only embeds the data set \cite{ham2004kernel}. However, Silva and Tenenbaum have shown that MDS's result can be used to reduce new samples (L-MDS) \cite{silva2002global}. More will be discussed ahead, when L-ISOMAP is presented.

Figure \ref{fig:comparison_oriiso_skiso} displays the reduction of the Swiss-roll data set done by the original implementation of ISOMAP (using MDS) and scikit-learn's implementation (using Kernel PCA). Numerically, they are identical (i.e., save from difference which tends to $0$).

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{experiments/iso_comp_my_and_sklearn}
	\captionsetup{justification=centering}
	\caption{The embedded swiss-roll data set by the original and scikit-learn's implementation.}
	\label{fig:comparison_oriiso_skiso}
\end{figure}

\subsubsection{Incremental ISOMAP}

The original ISOMAP algorithm reduces the data set in a single execution. Even its implementation with Kernel PCA will not update the reduction model if new data points are given, only reduce these points using the eigenvectors previously found. Therefore, updating ISOMAP reduction model requires recalculating it at every new data ``batch", which is very costly.

In order to allow ISOMAP to continuously learn from a stream of data, Incremental ISOMAP was proposed \cite{law2006incremental}. Relying on the fact that new data points will unlikely afect a large subset of vertices, it selectively updates the structures kept by ISOMAP. The update is performed as follows.

Let $[\vect X]_{n \times f}$ be a data set with $n$ samples and $f$ features, $\delta_{\vect a \vect b}$ be the dissimilarity measure between two samples $\vect a$ and $\vect b$, $\vect x_i$ be a sample in the nearest neighbor graph, $\vect x_j$ a neighbor of $\vect x_i$ such that their dissimilarity is higher among all $\vect x_i$'s neighbors. Additionally, let $[\vect Y]_{n \times d}$ be the embedding of $\vect X$ in the $d$-dimensional space. Finally, consider $\vect x_{n+1}$ a new sample to increment in the reduction model.

\begin{enumerate}
	\item The new sample $\vect x_{n+1}$ will replace $\vect x_j$ as a neighbor of $\vect x_i$ if $\delta_{\vect x_i \vect x_{n+1}} < \delta_{\vect x_i \vect x_j}$. A list of added or removed edges should be stored.
	
	\item The update of the shorest-path graph:
	\begin{enumerate}
		\item For every edge $(\vect x_i, \vect x_j)$ removed, the shortest-path between any two vertices $(\vect x_a, \vect x_b)$ that contains $(\vect x_i, \vect x_j)$ must be updated. That involves applying a slightly modified Dijsktra's algorithm, which will focus on finding the paths from a given source $\vect u$ and its unreached destination vertices, i.e., vertices $C(\vect u)=\{\vect c_i\}$ whose  shortest-paths $(\vect u, \vect c_i)$ contained a removed edge.
		
		\item The geodesic distances between all vertices and $\vect x_{n+1}$ must then be calculated. Let $A$ be the set of added edges:
		\begin{align*}
			g[n+1, i] = g[i, n+1] = \min_{j \mid (n+1, j) \in A} (g[i, j]+w(j, n+1)),\forall i.
		\end{align*}
		
		All the added edges are incident on vertex $\vect x_{n+1}$, naturally.  Calculating the shorest-path consists on considering two whichever edges $(\vect a, \vect x_{n+1}), (\vect x_{n+1}, \vect b)$ and relaxing the edge $(\vect a, \vect b)$ by considering the path $\vect a \to \vect x_{n+1} \to \vect b$.
	\end{enumerate}
	
	\item Finally, the eigenvectors/eigenvalues of the inner product matrix $\vect B_{new}$ (which contains the recently added sample $\vect x_{n+1}$) can be found through an iterative scheme, where ``a good initial guess for the subspace of dominant eigenvectors is the column space of $\vect Y$" \cite{law2006incremental} (the embedding before the $\vect x_{n+1}$ increment).
	A better eigen-space for $\vect B_{new}$, and thus $\vect Y_{new}$ can be found by Subspace iteration together with Rayleigh-Ritz acceleration \cite{law2006incremental}:
	\begin{enumerate}
		\item Let $\vect Z = \vect B_{new} \vect Y$. Perform QR-decomposition on $\vect Z$: $\vect{Z = B_{new}Y = QR}$.
		
		\item Take $\vect{V=Q}$ (remember that $\vect Q$ is intrinsically an orthogonal matrix) and $\vect{Z^*=V^\top} \vect B_{new} \vect V$. Perform spectral-decomposition of $[\vect Z^*]_{d \times d}$ to find $\vect \Lambda = diag(\lambda_i)$ and $\vect V = [\vect v_i]_{d \times d}$. This is significantly faster than directly decomposing $\vect B_{new}$, considering the dimensions of both matrices. 

		\item $\vect Y_{new} = [\vect Y, \vect y_{n+1}]^\top$, where $\vect y_{n+1}$ is the $d$-dimensional embedding of $\vect x_{n+1}$:
		\begin{align*}
			\vect y_{n+1} &= (\frac{1}{\sqrt{\lambda_1}} v_1 \cdot f, \dots, \frac{1}{\sqrt{\lambda_d}} v_d \cdot f) \\
			2f_i &\approx \frac{1}{n}[\sum_j g_{ij}^2 - \sum_{lj} g_{lj}^2 + \sum_l g_{l, n+1}^2] -g_{i, n+1}^2
		\end{align*}
	\end{enumerate}
\end{enumerate}

In regard to time complexity, considering $q$ the maximum degree of the vertices in the graph, $F$ is the set of pairs whose shortest path contained a removed edge and $H$ is the set of vertex pairs whose geodesic distances were modified, Incremental ISOMAP will require $O[q(|F|+|H|)]$ time steps in phase 1; $O[n^2(\log n + q)]$ in 2; and $O(n^2)$ in 3. The time complexity of the algorithm is, therefore, $O[q(|F|+|H|) + n^2(\log n + q + 1)]$.

\subsubsection{L-ISOMAP}
\label{sec:lisomap}

ISOMAP's time complexity dependents mainly on the number of samples. Remember: for $N$ samples, ISOMAP will require $O[n^2(\log n + k)]$ and $O[n^3]$ time operations to compute the shortest-paths and eigenvectors/values, respectively.

Landmark ISOMAP aims to reduce the complexity of the algorithm by finding the dissimilarities between $n$ landmarks (a small subset of samples) and all the original samples. Its complexity can then be reduced to $O[n^2(\log N + k) + n^2N]$. If $n \ll N$, this represents a expressive performance improvement \cite{silva2002global}.

Let $\vect X$ be the initial data set of interest, $l$ be the dimensionality to which $\vect X$ should be reduced and $\epsilon > 0 \in \mathbb{N}$ a number to ``ensure stability" \cite{silva2002global}. L-ISOMAP is defined as it follows.

\begin{enumerate}
	\item Select $n$ random samples from $\vect X$, such that $n > l + 1 + \epsilon$.
	\item Calculate the nearest-neighbors of each selected samples (all the original samples should be considered as neighbors).
	\item Calculate the shortest-paths from the neighborhood graph obtained in the previous step and the dissimilarity matrix $[\vect \delta]_{n \times N}$.
	\item Perform Landmark MDS, a slightly modified MDS:
	\begin{enumerate}
		\item Apply classical MDS to the submatrix of landmarks $[\vect \delta]_{n \times n}$.
		\item Embbed the remaining points through:
		\begin{align*}
			\vect y &= -\frac{1}{2} L^{-1}  (\vect \Delta_x - \bar{\vect \Delta}_n)\\		
			\vect y &= -\frac{1}{2} \colvec{4}{\frac{v_1^T}{\sqrt{\lambda_1}}}{\frac{v_2^T}{\sqrt{\lambda_2}}}{\dots}{\frac{v_n^T}{\sqrt{\lambda_n}}} (\vect \Delta_x - \bar{\vect \Delta}_n)\\
        \end{align*}
		Where $\vect \Delta_x$ is the square dissimilarity column vector from sample $\vect x$ to all landmarks and $\bar{\vect \Delta}_n$ the vector containing the column mean of $\vect \Delta_n$ (the square dissimilarity matrix between landmarks).
	\end{enumerate}
\end{enumerate}

\subsubsection{p-ISOMAP}

The embedding found by ISOMAP is strongly depentend of the parameters $k$ or $\epsilon$. When these parameters are too small, the neighborhood graph extracted might be disconnected, causing the dissimilarity between some vertices to be infinite. On the other hand, when the parameters are too big, vertices from different neighborhoods are prone to stay connected, preventing the manifold to be ``unfolded". Hence the importance of choosing appropriate parameters. However, in many cases, these ``appropriate" paramaters might not be evident. Hence the interest of developing a parameterless ISOMAP.

p-ISOMAP is an extension that automatically selects the parameters $k$ or $\epsilon$ \cite{choo2010p}. Assuming an result of ISOMAP for a particular parameter value is available, a new parameter will be considered and p-ISOMAP will iteratively update the soluton by:
\begin{enumerate}
	\item Adding/removing neighbors for every vertex in the neighborhood graph regarding the new parameter. This operation has time complexity $O(n \max_i |\vect \Delta e_i|)$, where $|\vect \Delta e_i|$ is the number of inserted/removed edges associated with sample $\vect x_i$.
	
	\item Update the shortest-paths described by the pairs of vertices in $F \subset Q$, where $Q$ is the set of all pairs of vertices and $F$ is the set of pairs associated with the updated edges in the previous step.
	\begin{enumerate}
		\item If the parameter value increased in comparison to the previous one, edges might have been added. Updating the shortest-path tree consists in simply relaxing edges associated with any of the vertices in the pairs of $F$. The time complexity is loosely bounded by $O(|A|q|F|)$, where $|A|$ is the number of inserted edges and $q$ is the maximum degree of vertices in G.
		
		\item If the parameter was decreased, then edges might have been removed and all pairs of vertices whose shortest-paths contain those edges are added to $F$. Identifying $F$ can be done in $O(n^2)$ times steps to execute, given some optimzation (see \cite{choo2010p}). Now, Dijkstra's can be selectively performed, requiring $O(n \max_i (|E_i'| \log |V_d(i)| + |E_i''|)$, where, given a vertex $\vect x_i$, $V_d(i)=\{\vect x_j | (\vect x_i, \vect x_j) \in F\}$, $E_i'$ is the set of edges connecting whichever pair of vertices in $V_d(i)$ and $E_i''$ is the set of edges connecting the vertices $\vect x_a$ and $\vect x_b$ such that $\vect x_a$ is in $V_d(i)$ and $\vect x_b$ is not.
	\end{enumerate}

	\item Finally, the last step is to update the eigenvectors and eigenvalues associated with the matrix $\vect B$. They can be approximated using the Lanczos algorithm, an iterative refining method that takes an initial solution candiadate. Officially, the time complexity of Lanczoz algorithm is $O(n^2)$ \cite{chen2004machine}. Yet, its timeframe for convergence also strongly depends on the solution candidate given. Assuming the previous inner matrix and the current one are similar, one might use the previous eigenvalues/vectors found as candidate. In this scenario, Lanczoz's will likely finish much faster than other algorithms.
\end{enumerate}

The experiments presented in \cite{choo2010p} have shown p-ISOMAP consistently maintaining a better performance than regular ISOMAP for data sets with a samples count varying between 500 and 3500. Additionally, the experiments have also demonstrated how p-ISOMAP performance is sensible to high variances of $k$ and $\epsilon$ values.

\subsection{Evaluating Reductions}

ISOMAP's aim is to ``unfold" the data set and, of course, it might distort dissimilarities between samples in the process. Hence drastically increasing Kruskal's stress. Clearly, this measure is unsuitable for evaluating ISOMAP. As proposed by L. Shi and J. Gu, Kruskal's stress can be easily updated to match ISOMAP nature. That is, to preserve dissimilarities within neighborhoods \cite{shi2012fast}:

Let $\vect X_{n \times f}$ be a data set with $n$ samples and $f$ dimensions, $\vect Y_{n \times p}$ its reduction to $p$ dimensions, and $\delta_{ij}$ and $\hat{\delta}_{ij}$ the dissimilarity measurements defined for all samples $\vect i$ and $\vect j$ in both $\vect X$ and $\vect Y$. Furthermore, given a sample $\vect i$ and $\beta \in \mathbb{N} \mid \beta \le n$, take $\Omega=K \bigcup \Theta$, where $K$ is the set containing the nearest neighbors of $i$ and $\Theta$ is a set of random samples such that $|\Theta| \le \beta \le n$:
\begin{align*}
	Stress_{ISOMAP} &= [\frac{\sum_i \sum_{j \in \Omega} (\delta_{ij} - \hat{\delta}_{ij})^2}{\sum_i \sum_{j \in \Omega} \delta_{ij}^2}]^{\frac{1}{2}}
\end{align*}

\subsection{Applicability and Limitations}

From figure \ref{fig:mani_s_reduced} and the experiment shown in section \ref{sec:experiments}, it is quite clear that ISOMAP outperforms PCA on the data set $\vect S$. Unfortunately, this cannot be projected for a generic case considering the strict and artificial nature of $\vect S$. In real-world data sets, the application of ISOMAP may lead to poor low-dimensional embeddings \cite{herik2009}. Bellow are listed some of the issues that have great influence over ISOMAP's results:
\begin{description}
	\item[Manifold Assumption] It refers to the initial assumption that the data lies on a low-dimensional manifold. Although wildly exploited by many authors, it is difficult to assert whether such assumption holds or not in real-world data sets \cite{lin2008riemannian}. Furthermore, even if the data roughly lies on a manifold, discontinuities in the data pattern can characterize the manifold as non-smooth. In these situations, graphs with edges that disrespect locality would be extracted and, hence, poor low-dimensional representations are likely to be produced.

	\newpage

	\item[Manifold Convexity] ISOMAP assumes that the data is geodesically convex \cite{donoho2002does}. This issue becomes clear when dealing with data sets with ``holes" in it that occupy sufficiently large areas, resulting in a neighborhood graph with great disconnected areas that require paths with great curvature to get around. Lerman demonstrated how reducing non-convex data sets with ISOMAP can easily yield distorted results \cite{lerman2005}:
	\begin{figure}[H]
		\centering
		\begin{subfigure}[H]{.45\linewidth}
			\includegraphics[width=\linewidth]{img/convexity/original.png}
			\captionsetup{justification=centering}
			\caption{The non-convex Swiss-roll \protect\footnotemark[6].}
		\end{subfigure}
		~
		\begin{subfigure}[H]{.45\linewidth}
			\includegraphics[width=\linewidth]{img/convexity/expected.png}
			\captionsetup{justification=centering}
			\caption{The expected reduction \protect\footnotemark[6].}
		\end{subfigure}
		\newline\newline

		\begin{subfigure}[H]{.45\linewidth}
			\includegraphics[width=\linewidth]{img/convexity/actual.png}
			\captionsetup{justification=centering}
			\caption{The actual reduction \protect\footnotemark[6].}
		\end{subfigure}
	\end{figure}
	
	\footnotetext[6]{From ``Manifold Learning Techniques: so which is the best?" by G. Lerman, \textit{University of California, Los Angeles}. Available at:
	\url{http://math.ucla.edu/~wittman/mani/mani_presentation.pdf}.}

	\newpage
	\item[Noise] Noise in data is expressed by samples that somehow diverge from their neighborhood (outliers). As these samples become farthest from its original neighborhood, the chances of being linked to samples from other neighborhood increase, possibly decreasing the quality of the solution. A possible solution is to remove these samples during the pre-processing stage \cite{herik2009}.

	The image bellow illustrates an attempt to use ISOMAP to reduce the $\vect Swiss_{n=.4}$, which is the Swiss-roll data set subjected to a noise factor of $.8$:

	\begin{figure}[H]
		\centering
		\begin{subfigure}{.40\linewidth}
			\centering
			\includegraphics[width=\linewidth]{img/noise/sr/1}
			\captionsetup{justification=centering}
			\caption{The data set $\vect{Swiss}_{n=.8}$.}
		\end{subfigure}~
		\begin{subfigure}{.35\linewidth}
			\centering
			\includegraphics[width=\linewidth]{img/noise/sr/3}
			\captionsetup{justification=centering}
			\caption{The neighborhood graph extracted from $\vect{Swiss}_{n=.8}$.}
			\label{fig:noise_3}
		\end{subfigure}
		\newline\newline\newline
		\begin{subfigure}{.45\linewidth}
			\centering
			\includegraphics[width=\linewidth]{img/noise/sr/2}
			\captionsetup{justification=centering}
			\caption{$\vect{Swiss}_{n=.8}$ reduced.}
			\label{fig:noise_2}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{The ISOMAP applied on a noisy data set.}
	\end{figure}

	Observe how some outliers were sufficiently far from their original neighborhood in the graph \ref{fig:noise_3}, to the point that Nearest-Neighbor Search maintained the edge between them and samples of completely different colors. MDS then attempted to maintain this dissimilarity, resulting in a deformed reduction (figure \ref{fig:noise_2}).
\end{description}
