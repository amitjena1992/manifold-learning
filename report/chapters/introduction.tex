\chapter{Introduction}

Throughout the years, machine learning techniques have grown popular between both academical and the corporative sectors. Their vast applications and promising results \cite{brownlee} indubitably contributed to our current scenario where not only computer scientists or mathematicians, but engineers, psychologists and many other groups have taken interest \cite{baldi2001bioinformatics} on how to adapt and apply these studies to their own problems.

Machine learning can help us to understand large amount of data and take decisions based on it. To achieve this, however, we must first find ways to effectively (and efficiently) extract the information that lies within the data.

Many different machine learning algorithms have been developed during this century and the last one. Among those, many could successfully generalize low dimensional data \cite{wang2012geometric}. In the other hand, problems of our world are often too complex and may be represented by high dimensional data. For example, images, sounds or text documents can be expressed as vectors of the $\mathbb{R}^n$, where each element corresponds to a pixel, wave signal or term, respectively. When analyzing these problems, we observed that many of the algorithms would often become unstable. Dimensionality reduction (or DR) then quickly became a key concept for minimizing the data size while maintaining its meaning.

Nowdays, dimensionality reduction has evolved into an extensive area. Being approached by many different perpectives, it can not only be applied to reduce the data size, specifically, but often employed in data preprocessing, visualization, noise reduction and many other purposes. Nonetheless, the area still presents questions and great challenges to be solved. For example, it is still difficult to evaluate a reduction or even define generic metrics for it, as the ``correctness" of a reduction is always associated with the specific problem domain in hand. Another issue is the difficulty attached to reduce nonlinearly distributed data sets.

In this work, we will focus on Isometric Feature Mapping (ISOMAP), a classic algorithm for Manifold Learning and highly regarded for nonlinear dimensionality reduction given its effectiveness when it has its pre-conditions met. First, the relevant background will be presented in order to contextualize the reader and provide concepts that are closely related to machine learning, dimensionality reduction and, of course, the ISOMAP algorithm. We will proceed to study linear dimensionality reduction, its algorithms, applications and limitations. Following, ISOMAP will be covered: its concept, computational complexity, extensions and limitations. Finally, experiments created during the project are presented for both observation and comparison.
